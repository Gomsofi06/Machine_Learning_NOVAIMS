{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bcee083-80e0-4a06-860a-de8351bfa268",
   "metadata": {},
   "source": [
    "# <center>Machine Learning Project</center>\n",
    "\n",
    "** **\n",
    "## <center>*06 - Predictive Model*</center>\n",
    "\n",
    "** **\n",
    "\n",
    "The members of the `team` are:\n",
    "- Ana Farinha - 20211514\n",
    "- Francisco Capontes - 20211692\n",
    "- Sofia Gomes - 20240848\n",
    "- Rui Louren√ßo - 2021639"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef9fbb0-e5cd-4b8b-9b43-917aa2930313",
   "metadata": {},
   "source": [
    "\n",
    "| Model              | Best Parameters  | Feature Selection | Average Validation Macro F1 Score | Validation Macro F1-Score |\n",
    "|--------------------|--------------------------------------|---|---------------|---------------|\n",
    "| CatBoostClassifier | `{\"iterations\": 1000, \"learning_rate\": 0.11, \"depth\": 6, \"l2_leaf_reg\": 5, bagging_temperature\": 0.4}`| `None` | `0.41` | `0.42` |\n",
    "| XGBoostClassifier  | `{\"n_estimators\": 200, \"learning_rate\": 0.2, \"max_depth\": 7, \"subsample\": 0.9, \"colsample_bytree\": 0.9, \"gamma\": 0.3}`| `None` | `0.42`|`0.42`|\n",
    "| Decision Trees     | `{\"min_samples_split\": 10, \"min_samples_leaf\": 4, \"max_depth\": 20, \"criterion\": \"entropy\"}`| `Essential Features` | `0.33`| `0.34`|\n",
    "| Naive Bayes        | `Default Parameters`| `Essential Features` | `0.24`| `0.23`|\n",
    "| StackEnsemble      | `CatBoost Config 1, XGBoost Config 1, Decision Trees, Default Parameters`     | `Essential Features` | `0.30`| `0.31`|\n",
    "| VotingEnsemble     | `CatBoost Config 2, XGBoost Config 2, Decision Trees, Default Parameters` | `None` | `0.41`| `0.42`|\n",
    "| XGBoostClassifier With kfold | `{\"n_estimators\": 200, \"learning_rate\": 0.2, \"max_depth\": 7, \"subsample\": 0.9, \"colsample_bytree\": 0.9, \"gamma\": 0.3}`| `None` | `0.44`|`0.45`|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9092592-0607-479b-9c85-23f4eea4ddab",
   "metadata": {},
   "source": [
    "After many iterations of preprocessing, modeling and gridsearch we found that XGBoostClassifier was slightly more consistent than the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "68d971ae-afd5-44af-bfb1-d68601957cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#make the split here\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import time\n",
    "\n",
    "from utils import *\n",
    "from utils_feature_selection import check_performace\n",
    "from utils_dicts import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "random_state=68+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21023784-0a5b-4f1d-b236-a26832a01f0a",
   "metadata": {},
   "source": [
    "## <span style=\"color:salmon\"> 1. Import Dataset </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "78208177-053c-41bd-a7b1-bbef43db1b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datasets\n",
    "train_df = pd.read_csv('preprocessed_data/train_data.csv', index_col=\"Claim Identifier\")\n",
    "test_df = pd.read_csv('./preprocessed_data/test_data.csv', index_col = 'Claim Identifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17c75069-661f-4b8d-9160-ccae48dfc7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Feature Selection: essential_features, reduced_features or []\n",
    "feature_selection = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affd661e-4eeb-4d84-a5c0-151ff7070d37",
   "metadata": {},
   "source": [
    "Define y as a target \"Claim Injury Type Encoded\" and X with all the other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2232728-35e4-4e12-8ec3-30483b640b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop([\"Claim Injury Type Encoded\"], axis = 1)\n",
    "y = train_df[\"Claim Injury Type Encoded\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316f0ce6-5d2f-454a-aee1-dbfd85da9848",
   "metadata": {},
   "source": [
    "## <span style=\"color:salmon\"> 2. Model Training</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05dd071-d71a-4bee-9c24-1e712e00896a",
   "metadata": {},
   "source": [
    "Defining the configuration for the model and the class mapping for the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e040a1ea-e2fb-4b9e-a495-21f826772fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"n_estimators\": 200,\n",
    "    \"learning_rate\": 0.2,\n",
    "    \"max_depth\": 7,\n",
    "    \"subsample\": 0.9,\n",
    "    \"colsample_bytree\": 0.9,\n",
    "    \"gamma\": 0.3,\n",
    "    \"random_state\": random_state\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "84430fe0-83de-4730-8e57-e56cf1b44143",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = {\n",
    "    0:'1. CANCELLED', \n",
    "    1:'2. NON-COMP',\n",
    "    2:'3. MED ONLY', \n",
    "    3:'4. TEMPORARY',\n",
    "    4:'5. PPD SCH LOSS', \n",
    "    5:'6. PPD NSL', \n",
    "    6:'7. PTD', \n",
    "    7:'8. DEATH'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58646c0-fc43-4001-884e-282b69cb47a3",
   "metadata": {},
   "source": [
    "We decided that instead of using a XGBoost Classification Model with a train_test_split to train the final model, we could implemented a pipeline that creates the 6 (number of folds) versions of the same model trained in diferent segments of the training dataset using Stratified K-fold.\n",
    "\n",
    "For the pipeline we first split the data into training set and validation set, and create copies of the training set and test_df in order to correctly preprocess the data.\n",
    "\n",
    "The scalers and models from the pipeline were saved in order to be used on the GrantApp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "52f3f980-124a-4321-8c6b-2ca754dd6522",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=6, shuffle=True, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aeaff0-c520-4de4-a2c1-f2475506b9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Fold 1...\n",
      "\n",
      "Fold 1 train F1 score: 0.7488\n",
      "Fold 1 validation F1 score: 0.4336\n",
      "------------------------------\n",
      "Processing Fold 2...\n",
      "\n",
      "Fold 2 train F1 score: 0.7505\n",
      "Fold 2 validation F1 score: 0.4446\n",
      "------------------------------\n",
      "Processing Fold 3...\n",
      "\n",
      "Fold 3 train F1 score: 0.7552\n",
      "Fold 3 validation F1 score: 0.4567\n",
      "------------------------------\n",
      "Processing Fold 4...\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 57\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m     44\u001b[0m model \u001b[38;5;241m=\u001b[39m XGBClassifier(\n\u001b[0;32m     45\u001b[0m         n_estimators\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m\"\u001b[39m],        \n\u001b[0;32m     46\u001b[0m         learning_rate\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m],      \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m         verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m                                 \n\u001b[0;32m     56\u001b[0m     )\n\u001b[1;32m---> 57\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Save Model\u001b[39;00m\n\u001b[0;32m     59\u001b[0m model\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./OpenEnded/Model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\abdar\\anaconda3\\envs\\ML_Project\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abdar\\anaconda3\\envs\\ML_Project\\Lib\\site-packages\\xgboost\\sklearn.py:1531\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1511\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[0;32m   1512\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1513\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[0;32m   1514\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1528\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[0;32m   1529\u001b[0m )\n\u001b[1;32m-> 1531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[0;32m   1546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\abdar\\anaconda3\\envs\\ML_Project\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abdar\\anaconda3\\envs\\ML_Project\\Lib\\site-packages\\xgboost\\training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abdar\\anaconda3\\envs\\ML_Project\\Lib\\site-packages\\xgboost\\core.py:2101\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[0;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2100\u001b[0m     _check_call(\n\u001b[1;32m-> 2101\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[0;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2104\u001b[0m     )\n\u001b[0;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_preds = np.zeros((len(test_df), len(class_mapping)))\n",
    "avg_train = []\n",
    "avg_val = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X, y)):\n",
    "    print(f\"Processing Fold {fold + 1}...\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    test_temp =test_df.copy()\n",
    "    train_temp=X_train.copy()\n",
    "    \n",
    "    # Preprocess X_train and X_Val\n",
    "    remove_outliers(X_train)\n",
    "    X_train, X_val = apply_frequency_encoding(X_train, X_val, True, fold=fold)\n",
    "    NA_imputer(X_train,X_val, True, fold=fold)\n",
    "    create_new_features(X_train,X_val)\n",
    "    \n",
    "    # Preprocess Test_df\n",
    "    remove_outliers(train_temp)\n",
    "    train_temp, test_temp = apply_frequency_encoding(train_temp, test_temp)\n",
    "    NA_imputer(train_temp, test_temp)\n",
    "    create_new_features(train_temp, test_temp)\n",
    "\n",
    "    scaler = StandardScaler().fit(X_train[numerical_features])\n",
    "    # Save Scaler\n",
    "    joblib.dump(scaler, f'./OthersPipeline/Scaler_{fold}.pkl')\n",
    "    X_train[numerical_features]  = scaler.transform(X_train[numerical_features])\n",
    "    X_val[numerical_features]  = scaler.transform(X_val[numerical_features])  \n",
    "    test_temp[numerical_features]  = scaler.transform(test_temp[numerical_features])  \n",
    "\n",
    "    drop_list = []\n",
    "    if feature_selection != []:\n",
    "        for col in X_train.columns:\n",
    "            if col not in feature_selection:\n",
    "                drop_list.append(col)\n",
    "\n",
    "    X_train = X_train.drop(drop_list, axis=1)\n",
    "    X_val = X_val.drop(drop_list, axis=1)\n",
    "    test_temp = test_temp.drop(drop_list, axis=1)\n",
    "        \n",
    "    # Train model\n",
    "    model = XGBClassifier(\n",
    "            n_estimators=config[\"n_estimators\"],        \n",
    "            learning_rate=config[\"learning_rate\"],      \n",
    "            max_depth=config[\"max_depth\"],                          \n",
    "            subsample=config[\"subsample\"],              \n",
    "            colsample_bytree=config[\"colsample_bytree\"],\n",
    "            gamma=config[\"gamma\"],                     \n",
    "            objective=\"multi:softmax\",                  \n",
    "            num_class=8,                                \n",
    "            eval_metric=\"merror\",   \n",
    "            random_state=config[\"random_state\"],                                      \n",
    "            verbosity=0                                 \n",
    "        )\n",
    "    model.fit(X_train,y_train)\n",
    "    # Save Model\n",
    "    model.save_model(f\"./OpenEnded/Model_{fold}.json\")\n",
    "    \n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_val = model.predict(X_val)\n",
    "    \n",
    "    f1_train = f1_score(y_train, pred_train, average='macro')\n",
    "    f1_val = f1_score(y_val, pred_val, average='macro')\n",
    "\n",
    "    avg_train.append(f1_train)\n",
    "    avg_val.append(f1_val)\n",
    "\n",
    "    print(f\"Fold {fold + 1} train F1 score: {f1_train:.4f}\")\n",
    "    print(f\"Fold {fold + 1} validation F1 score: {f1_val:.4f}\")\n",
    "    print(f\"------------------------------\")\n",
    "    \n",
    "    test_preds += model.predict_proba(test_temp)\n",
    "\n",
    "print(f\"Average Train F1 score: {sum(avg_train)/len(avg_train)}\")\n",
    "print(f\"Average Validation F1 score: {sum(avg_val)/len(avg_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3dbbe7-b56b-4cd0-a80f-8ccaa2fc1170",
   "metadata": {},
   "source": [
    "## <span style=\"color:salmon\"> 3. Test Predictions </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e110af5-76f3-4843-ac6c-9f1dba18cff7",
   "metadata": {},
   "source": [
    "To calculate the **final prediction** we first averaged the test predictions of each fold by the number of total folds and then used **np.argmax** to select the class with the highest averaged probability.\n",
    "The final step was to create a dataframe with the index of test_df as **Claim Identifier** and the **Claim Injury Type** as the inversed mapping of the **final prediction** and **classes_mapping**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6f8f088-4797-44d6-a3d4-f654ef8e87ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = test_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21c0f48b-8031-4a2e-a948-a5177ff04a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_preds = np.argmax(test_preds / kf.get_n_splits(), axis=1)\n",
    "submission_df = pd.DataFrame({\n",
    "    'Claim Identifier': test_id,\n",
    "    'Claim Injury Type': final_test_preds\n",
    "})\n",
    "submission_df[\"Claim Injury Type\"] = submission_df[\"Claim Injury Type\"].replace(class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "410e922e-c323-4bf9-81be-1606a606da0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    version = version_control()\n",
    "    submission_df.to_csv(f'./submissions/Group49_Version{version:02}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
