{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Machine Learning Project</center>\n",
    "\n",
    "** **\n",
    "## <center>*03.10 - Gradient Boost*</center>\n",
    "\n",
    "** **\n",
    "\n",
    "The members of the `team` are:\n",
    "- Ana Farinha - 20211514\n",
    "- Francisco Capontes - 20211692\n",
    "- Sofia Gomes - 20240848\n",
    "- Rui Louren√ßo - 2021639\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#make the split here\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import time\n",
    "\n",
    "from utils import *\n",
    "from utils_feature_selection import check_performace\n",
    "from utils_dicts import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:salmon\"> 1. Import Dataset </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall thundersvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Please build the library first!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthundersvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVC\n",
      "File \u001b[1;32m~\\Desktop\\Masters\\Projects\\Machine_Learning_NOVAIMS\\env\\Lib\\site-packages\\thundersvm\\__init__.py:10\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m * Name        : __init__.py\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m * Author      : Locke <luojiahuan001@gmail.com>\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m * Version     : 0.0.1\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m * Description :\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthundersvm\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mthundersvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\Desktop\\Masters\\Projects\\Machine_Learning_NOVAIMS\\env\\Lib\\site-packages\\thundersvm\\thundersvm.py:52\u001b[0m\n\u001b[0;32m     50\u001b[0m         thundersvm \u001b[38;5;241m=\u001b[39m CDLL(lib_path)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease build the library first!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m SVM_TYPE \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc_svc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnu_svc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mone_class\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepsilon_svr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnu_svr\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     54\u001b[0m KERNEL_TYPE \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolynomial\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrbf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Please build the library first!"
     ]
    }
   ],
   "source": [
    "from thundersvm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "train_df = pd.read_csv('preprocessed_data/train_data.csv', index_col=\"Claim Identifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "test_df = pd.read_csv('./preprocessed_data/test_data.csv', index_col = 'Claim Identifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Feature Selection: essential_features, reduced_features or []\n",
    "feature_selection = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_percentage = train_df.isna().sum() / len(train_df) * 100\n",
    "for col, percent in missing_percentage.items():\n",
    "    if not percent == 0:\n",
    "        print(f\"{col}: {percent:.2f}% missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:salmon\"> 2. Prepare Dataset </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define y as a target \"Claim Injury Type Encoded\" and X with all the other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop([\"Claim Injury Type Encoded\"], axis = 1)\n",
    "y = train_df[\"Claim Injury Type Encoded\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:salmon\"> 3. Gradient Boosting</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config = {'max_depth': 5, 'learning_rate': 0.1, 'n_estimators': 200}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC()\n",
    "#        max_depth=config[\"max_depth\"],\n",
    "#        learning_rate=config[\"learning_rate\"],\n",
    "#        n_estimators=config[\"n_estimators\"],\n",
    "#        verbose = 0\n",
    "#    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_performace(model,X,y,numerical_features,essential_features,n_folds = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_performace(model,X,y,numerical_features,reduced_features,n_folds = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_performace(model,X,y,numerical_features,[],n_folds = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:salmon\"> 3.1  Evaluate the model </span> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X,y, test_size = 0.25, stratify = y, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val = apply_frequency_encoding(X_train, X_val)\n",
    "NA_imputer(X_train,X_val)\n",
    "create_new_features(X_train,X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train[numerical_features])\n",
    "X_train[numerical_features]  = scaler.transform(X_train[numerical_features])\n",
    "X_val[numerical_features]  = scaler.transform(X_val[numerical_features])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = [\"Average Weekly Wage\"]\n",
    "if feature_selection != []:\n",
    "    for col in X.columns:\n",
    "        if col not in feature_selection:\n",
    "            drop_list.append(col)\n",
    "X_train = X_train.drop(drop_list, axis=1)\n",
    "X_val = X_val.drop(drop_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = {\n",
    "    0:'1. CANCELLED', \n",
    "    1:'2. NON-COMP',\n",
    "    2:'3. MED ONLY', \n",
    "    3:'4. TEMPORARY',\n",
    "    4:'5. PPD SCH LOSS', \n",
    "    5:'6. PPD NSL', \n",
    "    6:'7. PTD', \n",
    "    7:'8. DEATH'\n",
    "}\n",
    "\n",
    "# Use the values from class_mapping as the target names\n",
    "target_names = list(class_mapping.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute confusion matrix to evaluate the accuracy of a classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_train, y_train_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_train, y_train_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_val_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_val_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:salmon\"> 4. Test Predictions </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make validation predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, test_df = apply_frequency_encoding(X, test_df)\n",
    "NA_imputer(X, test_df)\n",
    "create_new_features(X, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X[numerical_features])\n",
    "X[numerical_features]  = scaler.transform(X[numerical_features])\n",
    "test_df[numerical_features]  = scaler.transform(test_df[numerical_features])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = [\"Average Weekly Wage\"]\n",
    "if feature_selection != []:\n",
    "    for col in X.columns:\n",
    "        if col not in feature_selection:\n",
    "            drop_list.append(col)\n",
    "test_df = test_df.drop(drop_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make validation predictions\n",
    "y_test_pred = model.predict(test_df)\n",
    "y_test_pred = y_test_pred.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_final = np.array([class_mapping[i] for i in y_test_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = model.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({\n",
    "    'Claim Identifier': test_id,\n",
    "    'Claim Injury Type': y_test_final\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    version = version_control()\n",
    "    submission_df.to_csv(f'./submissions/Group49_Version{version:02}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
